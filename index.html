<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Starter Template for Bootstrap</title>

  <!-- Bootstrap core CSS -->
  <link href="dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="starter-template.css" rel="stylesheet">

  <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
  <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
  <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
          aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">Project name</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="#">Home</a></li>
          <li><a href="#intro">Introduction</a></li>
          <li><a href="#obj">Project Objective</a></li>
          <li><a href="#design">Design</a></li>
          <li><a href="#drawings">Drawings</a></li>
          <li><a href="#testing">Testing</a></li>
          <li><a href="#result">Result</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>

  <div class="container">

    <div class="starter-template">
      <h1>ASL Interpreter</h1>
      <p class="lead">ECE 5730 Final Project<br>By Shi Gu (sg2562), Lulu Htutt (lh543), and Sana Chawla (sc2347).</p>
    </div>

    <hr>
    <div class="center-block">
      <iframe width="640" height="360" src="https://www.youtube.com/embed?v=et91Gea6CPk" frameborder="0"
        allowfullscreen></iframe>
      <h4 style="text-align:center;">Demonstration Video</h4>
    </div>

    <hr id="intro">

    <div style="text-align:center;">
      <h2>Introduction</h2>
      <p style="text-align: left;padding: 0px 30px;">
        For our final project we designed and developed a glove based system that can detect the alphabets of the
        American Sign Language (ASL) using flex sensors and an IMU.
        We created
        this system to provide an affordable alternative to expensive commercial sign language translation systems,
        making communication more accessible for individuals who are non-verbal, hard of hearing, or deaf. The project
        uses a Raspberry Pi Pico to process sensor data from five flex sensors (one for each finger), contact sensors
        and an MPU6050 IMU
        to detect hand orientation, implementing threshold-based signal processing to detect the ASL alphabet.
      </p>
    </div>

    <hr id='obj'>

    <hr id='design'>

    <div>
      <h2 style="text-align:center;">High Level Design</h2>
      <h3>Rationale and Sources</h3>
      <h3>Hardware Design</h3>
      <p style="text-align: left;padding: 0px 30px;">
        Our hardware includes a glove with 3 different kinds of sensors for detecting gestures of each letter in ASL alphabet.
        The following is the picture of our final project:
      </p>
      <div style="text-align:center;">
        <img class="img-rounded" src="pics/glove_pic.jpeg" alt="Generic placeholder image" style="width:30%;">
        <h4>Picutre of the final project</h4>
      </div>
      <p style="text-align: left;padding: 0px 30px;">
        We placed five flex sensors on the top of each finger to detect individual finger bending. 
        The resistance of a flex sensor increases when it is bent, allowing it to act as a variable resistor.
        By connecting each sensor in a voltage divider configuration with a pull-down resistor, 
        we can convert the resistance change into a measurable voltage. 
        The output voltage is then read by the microcontroller to determine the degree of bending. 
        To maximize sensitivity, we selected the pull-down resistor values based on the unbent resistance of each flex sensor. 
        One sensor had a much higher baseline resistance (~90 kΩ), while the others measured closer to ~60 kΩ. 
        We chose resistor values that are comparable to these resistances to ensure a larger voltage swing in response 
        to bending, which is helpful for our detection accuracy.
      </p>

      <p style="text-align: left;padding: 0px 30px;">
        For reading all 5 values of each flex sensor, we used an external 10-bit 8-channel Analogue-to-Digital Converter (ADC) with SPI protocol.
        Raspberry Pi Pico with RP2040 has 3 ADC reading pins which is not enough for 5 readings. Since gesture detection 
        does not need high-speed or real-time sampling, we used an 8-channel ADC which has an internal multiplexer to 
        sequentially return readings of each ADC channel.
      </p>

      <p style="text-align: left;padding: 0px 30px;">
        We used a Inertial Measurement Unit (IMU), specifically MPU6050, to detect the rotation and motion of the hand.
        It was placed at the center of the back of the glove and communicates with Raspberry Pi via I2C protocol. 
        The IMU contains both an accelerometer and a gyroscope. 
        The accelerometer provides acceleration readings along three axes, which can be used to estimate orientation, 
        while the gyroscope measures angular velocity along the same axes to detect motion.
        Both types of data are essential for accurate gesture recognition because some letters have the same fingers bent but differ  
        in hand orientations, such as P and K, and some letters are dynamic such as J and Z.
      </p>

      <p style="text-align: left;padding: 0px 30px;">
        Contact sensors are employed to detect contacts. Below is the configuration of our contact sensors:
      </p>

      <div style="text-align:center;">
        <img class="img-rounded" src="pics/contact_sensor.png" alt="Generic placeholder image" style="width:30%;">
        <h4>Contact Sensor Configuration - Each circle represents a copper foil (Red indicates 3.3 V, and Green are GPIO inputs)</h4>
      </div>
      <p style="text-align: left;padding: 0px 30px;">
        The sensors are configured in a pull-up way. The red circles are copper foils wired to 3.3 V power pin, while the green circles are 
        copper foils connected to GPIO inputs. As red and green contacts, the GPIO input should return a HIGH reading, indicating two fingers contact.
        This is useful to identify letters that differ only in fingers contact such as U and V.
      </p>

      <p style="text-align: left;padding: 0px 30px;">
        Beside sensors for detecting gesture, VGA and a button is used to display the corresponding letters detected. We connected a Raspberry Pi debugger to 
        print detected letters on the serial monitor. And the user can press the button to print the letter detected to the VGA screen. Below is an example of 
        printed "HELLO WORLD".
      </p>

      <div style="text-align:center;">
        <img class="img-rounded" src="pics/vga_hello_world.jpeg" alt="Generic placeholder image" style="width:30%;">
        <h4>HELLO WORLD printed on VGA screen</h4>
      </div>

      <div style="text-align:center;">
        <img class="img-rounded" src="pics/actions_design.jpeg" alt="Generic placeholder image" style="width:80%;">
        <h4>Actions Designs</h4>
      </div>
      <h3>Software Design</h3>
      <p style="text-align: left;padding: 0px 30px;">
        We decided to use a thresholding based system for detection

        In the backend, we used Pygame, OpenCV, and mySQL/Sqlite. We created the game database with three
        tables: Users, Tamagotchis, and Relations. Users have information such as a UID and an image path for
        the facial recognition. The tamagotchis table had every possible tamagotchi stored, with information
        such as Tamagotchi ID, name, and image. The relations table was used to keep track of which users had
        which tamagotchis, and the specific statistics (age, health, hunger, happiness) of these tamagotchis. We
        had to set the primary key to the pair (UID, TID) because each entry would had a unique user and
        corresponding tamagotchi.
      </p>
      <p style="text-align: left;padding: 0px 30px;">
        For the facial recognition, we used OpenCV, specifically the face_recognition library. Using the
        Pi-camera, the user would take a photo of themself. New users would save this photo in the user_images
        folder and would be inserted into the database. Returning users or users who choose to interact with
        their tamagotchi need to be recognized. After taking a photo, we use the face_recognition library's
        face_encodings function to extract the features of the face. Then, we compare it to the features of the
        faces in the database and take the Euclidean distance of these pairings. Finally, using argmin, we
        select the user with the smallest distance.
      </p>
      <p style="text-align: left;padding: 0px 30px;">
        Pygame was used to tie all of the features together. We used it to take the user's interactions with the
        interface and update the screen accordingly. This included pulling data from the database, updating it
        when feeding/cleaning, turning on the camera when needed, and handling menu navigation.
      </p>
      <p style="text-align: left;padding: 0px 30px;">
        A feature we wanted to add as an extra was an emotion detection interaction, where the tamagotchi would
        play a sound or get happier/sadder depending on the user's emotion. We were able to train an emotion
        detection model with ~76% accuracy on our local machines. However, we ran into many issues when trying
        to install the tensorflow and keras libraries on the Raspberry-Pi. The version of tensorflow available
        for the os architecture was only available with Python 3.7, whereas we had Python 3.9. One solution we
        found was to convert the model to a TFLite model, as we were able to successfully install TFLite on the
        Raspberry-Pi. Unfortunately, after many attempts, we could not convert the model. The code and model can
        be found in the github repository.
      </p>
      <h3>Hardware</h3>
      <p style="text-align: left;padding: 0px 30px;">
        We used the PiTFT to show the game screen, the Pi-camera to take photos, and the four GPIO pins on the
        Raspberry-Pi. The four pins used were 17, 22, 23, and 27, which were mapped to the up, down, select, and
        quit functions, respectively. Because the original Tamagotchi system was handheld, we created a
        cardboard shell that looked like a house. The user could open a "door" to the PiTFT screen, similar to
        having a pet at home.
      </p>
    </div>

    <hr id='drawings'>

    <div style="text-align:center;">
      <h2>Results of the Design</h2>
      <p style="text-align: left;padding: 0px 30px;">
        We illustrated our Tamagotchis, status bars, and tombstone using pixel art.
      </p>
      <div class="row" style="text-align:center;">
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama1.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Capz</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama2.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Pacmen</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama3.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Jaredz</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama4.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Bob</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama5.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Kin gJuilan</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama6.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Jonathon</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama7.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Nola</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama8.png" alt="Generic placeholder image" width="100" height="100">
          <h5>NYSEG</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama9.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Epty</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama10.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Penguin</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tama11.png" alt="Generic placeholder image" width="100" height="100">
          <h5>Fish</h5>
        </div>
      </div>
      <h4>Tamagotchis</h4>
      <div class="row" style="text-align:center;">
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/happy_bar.png" alt="Generic placeholder image" width="50" height="50">
          <h5>Happiness</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/health_bar.png" alt="Generic placeholder image" width="50" height="50">
          <h5>Health</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/hunger_bar.png" alt="Generic placeholder image" width="50" height="50">
          <h5>Hunger</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/pizza.png" alt="Generic placeholder image" width="50" height="50">
          <h5>Pizza</h5>
        </div>
        <div class="col-md-2" style="font-size:16px">
          <img class="img-rounded" src="pics/tombstone.png" alt="Generic placeholder image" width="100" height="140">
          <h5>Death</h5>
        </div>
      </div>
      <h4>Other Icons</h4>
    </div>

    <hr id='testing'>

    <div style="text-align:center;">
      <h2>Conclusion</h2>
      <p style="text-align: left;padding: 0px 30px;">
        Our testing strategy was to isolate components/features of our code and get them to work before
        combining them into the main game code. For facial recognition, we did testing using the camera.py file
        and a separate testing database, and once it was working, we added the code into game.py and changed the
        SQLite commands to interact with the main tamagotchi.db. One of our initial concerns was how accurate
        the model would be, but after testing on 11 people, it only misclassified one person one time. All game
        functionality testing was done by just running the code and checking print statements if something
        wasn't working as intended.
      </p>
    </div>

    <hr id='result'>

    <div style="text-align:center;">
      <h2>Result</h2>
      <p style="text-align: left;padding: 0px 30px;">
        For our demo, we had a little trouble getting detecting the letter R.
      </p>
    </div>

    <hr>

    <hr>
    <div style="font-size:18px">
      <h2>Parts List</h2>
      <ul>
        <li>Raspberry Pi Pico</li>
        <li>IMU</li>
        <li>Flex Sensors</li>
        <li>Glove</li>
        <li>Copper contact sensors</li>
        <li>8 channel ADC</li>
      </ul>
      <h3>Total: $66.74</h3>
    </div>
    <hr>
    <div style="font-size:18px">
      <h2>References</h2>
      <a href="https://www.sparkfun.com/flex-sensor-2-2.html">Flex Sensors</a><br>
      <a href="https://invensense.tdk.com/wp-content/uploads/2015/02/MPU-6000-Datasheet1.pdf">IMU Datasheet</a><br>
      <a href="https://cdn-shop.adafruit.com/datasheets/MCP3008.pdf">8 Channel ADC</a><br>
    </div>

    <hr>

    <div class="row">
      <h2>Code Appendix</h2>
      <p style="text-align: left;padding: 0px 30px;">
        Code can be found on <a href="https://github.com/luluhtutt/Pi-Tamagotchi">Github</a>
      </p>
    </div>

    <div class="row">
      <h2>Appendix A</h2>
      <p style="text-align: left;padding: 0px 30px;">
        The group approves this report for inclusion on the course website.
        The group approves the video for inclusion on the course youtube channel.
      </p>
    </div>

    <div class="row"></div>
      <h2>Appendix B</h2>
      <p style="text-align: left;padding: 0px 30px;">
        Schematics of our system:
      </p>
    </div>


  </div><!-- /.container -->




  <!-- Bootstrap core JavaScript
    ================================================== -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
  <script src="dist/js/bootstrap.min.js"></script>
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
</body>

</html>